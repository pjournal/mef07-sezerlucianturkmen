[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SEZER TURKMEN Progress Journal",
    "section": "",
    "text": "Introduction\n\nThis progress journal covers SEZER TURKMEN / YELLOW SUBMARINE’s work during their term at BDA 503 Fall 2023.\nEach section is an assignment or an individual work."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  Assignment 1",
    "section": "",
    "text": "2 | ABOUT ME\nGreetings!\nI’m Sezer Turkmen, a software developer whose journey spans the realms of architecture and coding. In a nutshell:\n\n2.0.1 From Architect to Code Creator: My Transition\nI initially ventured into the world of architecture, my first passion. My studies took me to Italy, where I delved into multi-scale architecture and sustainable design, honing my eye for detail and love for harmonious structures.\nAfter working as an architect for a considerable period, I felt the magnetic pull towards coding. The allure of shaping digital landscapes was irresistible, prompting a career shift.\n\n\n2.0.2 Mastering Full Stack Development\nAs a software developer, I immersed myself in coding, mastering Java and venturing into full-stack development. I used technologies like Spring Boot and React to craft dynamic, user-friendly applications. Kubernetes and Docker added efficiency and scalability to my toolkit, bridging the worlds of traditional and digital architecture.\nMy journey is guided by the belief in perpetual learning. I attended a coding bootcamp to refine my skills and gain practical insights, cementing my passion for coding. As a software developer, I enjoy creating elegant, user-centric applications.\nCurrently, I’m pursuing a master’s in Information Technologies at MEF University, ensuring my knowledge stays cutting-edge.\n\n\n2.0.3 Life Beyond Tech\nOutside of coding and architecture, I find joy in cooking, a creative outlet where flavors and textures are my canvas. Staying active is another passion. Whether it’s hitting the gym or enjoying indoor cycling with a spirited playlist, I cherish the rhythm and precision of physical activity.\n\n\n2.0.4 The Future Awaits\nI’m Sezer Turkmen, a full-stack developer, architect, and a lover of life. My mission is to blend creativity and technology, one line of code at a time. Thank you for getting to know me!\n  \n\n\n\n\n3 | DISCOVER THE POSIT\n\n3.0.1 YouTube Channel | The open-source data science company for the individual, team and enterprise.\n\n\n\n\nTeaching R online with RStudio Cloud\nI chose to watch the video titled “Mine Çetinkaya-Rundel | Teaching R online with RStudio Cloud” for two main reasons. Firstly, I was interested in learning about effective online teaching methods for the R programming language, commonly used in data science. The video delves into the benefits of using RStudio Cloud for teaching, emphasizing its user-friendly approach. Additionally, I was drawn to the fact that the presenter, Mine Çetinkaya-Rundel, shares a Turkish background with me, which added a unique cultural perspective to the subject matter. The presentation covers tips and best practices for using RStudio Cloud effectively in the classroom, as exporting and grading student work and sharing data files.\n\n\n\n\n4 | PROPOSE DATASET\n\n4.0.1 Most Streamed Spotify Songs 2023\nThe ‘Most Streamed Spotify Songs 2023’ dataset is a fantastic choice for class demos. It offers insights into the music industry by showcasing popular songs and their attributes. Students can explore the science of hit songs, understand their impact through social media analysis, and learn about cross-platform music data. It’s engaging and up-to-date, making it an excellent teaching resource.\n\n# Use Google Drive to connect Dataset with library of 'googleDrive'\nid &lt;- \"1nM0Cr-gVFF9UjvxSZXHNfFnrDWIUoTHi\"\ntotal_list &lt;- read.csv(sprintf(\"https://docs.google.com/uc?id=%s&export=download\", id))\n\n# Filter out rows with non-numeric values in the 'streams' column\nfiltered_spotify_data &lt;- total_list %&gt;%\n   filter(!grepl(\"[a-zA-Z]\", streams))\n\n# Sort the filtered dataset by the 'streams' column in descending order\nsorted_filtered_spotify_data &lt;- filtered_spotify_data %&gt;%\n  arrange(desc(streams))\n\n# Create HTML view of Dataset\nDT::datatable(head(sorted_filtered_spotify_data), editable = list(\n  target = 'row', disable = list(columns = c(1, 3, 4))\n))\n\n\n\n\n\n\n\nThe Most Streamed Spotify Songs 2023 dataset contains a comprehensive list of the most famous songs of 2023 as listed on Spotify. The dataset offers a wealth of features beyond what is typically available in similar datasets. It provides insights into each song’s attributes, popularity, and presence on various music platforms. The dataset includes information such as track name, artist(s) name, release date, Spotify playlists and charts, streaming statistics, Apple Music presence, Deezer presence, Shazam charts, and various audio features.\nGO KAGGLE FOR THIS DATASET\n\n\n\n\n5 | LEARN THREE R POST\n\n5.0.1 Creating Stunning Visuals with ggplot2 and ggExtra\nVisualizing the distribution of data is essential for understanding it better. In R, you can use ggplot2 and ggExtra to create informative charts.\nggplot2 is a popular tool for making all sorts of graphs. It’s user-friendly and versatile, letting you customize your plots easily.\nggExtra is like an upgrade for ggplot2, designed to make scatterplots and other graphs even more helpful. It adds histograms or density plots to the sides of your main chart, showing you how individual data points are spread out. This extra information helps you grasp your data’s patterns.\nTo use ggplot2 and ggExtra, first, make your main chart with ggplot2, and then use ggExtra’s function, ggMarginal(), to include those extra histograms or density plots. This adds depth to your visualizations, helping you understand data distributions better.\nIn a nutshell, ggplot2 and ggExtra are handy tools in R for exploring and presenting data, making your visualizations more informative and user-friendly.\n\n# library\nlibrary(ggplot2)\nlibrary(ggExtra)\n \n# classic plot :\np &lt;- ggplot(mtcars, aes(x=wt, y=mpg, color=cyl, size=cyl)) +\n      geom_point() +\n      theme(legend.position=\"none\")\n \n# Set relative size of marginal plots (main plot 10x bigger than marginals)\nggMarginal(p, type=\"histogram\", size=10)\n \n# Custom marginal plots:\nggMarginal(p, type=\"histogram\", fill = \"slateblue\", xparams = list(  bins=10))\n \n# Show only marginal plot for x axis\nggMarginal(p, margins = 'x', color=\"purple\", size=4)\n\n\n\n# Very basic chart\nggplot( mtcars , aes(x=mpg, y=wt)) + \n  geom_point()\n\n\n\n\nSEE MORE\n\n\n\n5.0.2 Text Analysis || Text Mining in R\nText analysis, or text mining, is a vital tool for extracting insights from unstructured text data. It’s essential because it helps us understand, categorize, and derive meaningful information from sources like social media, customer feedback, research papers, and more. For instance, it enables sentiment analysis to gauge public opinions, topic modeling for content organization, and efficient information retrieval from large datasets. In R, a popular choice for text analysis, you can use libraries like ‘tm’ and ‘tidytext’ to perform tasks like text preprocessing and visualizing word frequency, as demonstrated in the provided code. This skill empowers you to make data-driven decisions and save time by automating text processing tasks. Happy text mining!\n\nlibrary(stringr)\n#All functions in stringr start with str_ and take a vector of strings as the first argument:\n\nx &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_length(x) \n\n[1] 3 5 5 5 4 9\n\nstr_c(x, collapse = \", \")\n\n[1] \"why, video, cross, extra, deal, authority\"\n\nstr_sub(x, 1, 2)\n\n[1] \"wh\" \"vi\" \"cr\" \"ex\" \"de\" \"au\"\n\n#Most string functions work with regular expressions, a concise language for describing patterns of text. For example, the regular expression \"[aeiou]\" matches any single character that is a vowel:\n\nstr_subset(x, \"[aeiou]\")\n\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n\nstr_count(x, \"[aeiou]\")\n\n[1] 0 3 1 2 2 4\n\n#There are seven main verbs that work with patterns:\n\n# str_detect(x, pattern) tells you if there’s any match to the pattern:\n\nstr_detect(x, \"[aeiou]\")\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n#str_count(x, pattern) counts the number of patterns:\n\nstr_count(x, \"[aeiou]\")\n\n[1] 0 3 1 2 2 4\n\n#str_subset(x, pattern) extracts the matching components:\n\nstr_subset(x, \"[aeiou]\")\n\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n\n#str_locate(x, pattern) gives the position of the match:\n\n\nstr_replace(x, \"[aeiou]\", \"?\")\n\n[1] \"why\"       \"v?deo\"     \"cr?ss\"     \"?xtra\"     \"d?al\"      \"?uthority\"\n\n#str_split(x, pattern) splits up a string into multiple pieces:\n\n#fixed(): match exact bytes\n#coll(): match human letters\n#boundary(): match boundaries\n\nSEE MORE\n\n\n\n5.0.3 Random Forest in R\nThe randomForest package in R is a highly popular and effective tool for implementing ensemble learning techniques and constructing models based on decision trees. By harnessing the power of random forests, this package excels in producing reliable and precise predictions, making it well-suited for tasks involving high-dimensional datasets. Its versatility extends to both classification and regression challenges, where it can significantly enhance the quality of predictions by combining insights from multiple decision trees.\n\n#library(randomForest)\n#library(ggplot2)\n\nset.seed(4543)\ndata(mtcars)\nrf.fit &lt;- randomForest(mpg ~ ., data=mtcars, ntree=1000,\n                       keep.forest=FALSE, importance=TRUE)\n\nIn R, random forest regression permits parameter experimentation, such as adjusting ntree and mtry to assess their effects on residuals and variance. Importance metrics encompass mean square error (MSE) and node purity. For reliable variable importance evaluation, prioritize MSE. In cases where predictors are exclusively numerical, prioritize MSE when metrics diverge. Visualize importance results using varImpPlot(), and for enhanced insights, create a combined plot that accentuates MSE outcomes, providing a comprehensive view of variable importance in random forest regression models.\n\n# Get variable importance from the model fit\nImpData &lt;- as.data.frame(importance(rf.fit))\nImpData$Var.Names &lt;- row.names(ImpData)\n\nggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +\n  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color=\"skyblue\") +\n  geom_point(aes(size = IncNodePurity), color=\"blue\", alpha=0.6) +\n  theme_light() +\n  coord_flip() +\n  theme(\n    legend.position=\"bottom\",\n    panel.grid.major.y = element_blank(),\n    panel.border = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\nSEE MORE"
  },
  {
    "objectID": "inClass1.html",
    "href": "inClass1.html",
    "title": "2  In Class 1",
    "section": "",
    "text": "2.0.1 Most Streamed Spotify Songs 2023\nThe ‘Most Streamed Spotify Songs 2023’ dataset is a fantastic choice for class demos. It offers insights into the music industry by showcasing popular songs and their attributes. Students can explore the science of hit songs, understand their impact through social media analysis, and learn about cross-platform music data. It’s engaging and up-to-date, making it an excellent teaching resource.\n\n# Use Google Drive to connect Dataset with library of 'googleDrive'\nid &lt;- \"1nM0Cr-gVFF9UjvxSZXHNfFnrDWIUoTHi\"\ntotal_list &lt;- read.csv(sprintf(\"https://docs.google.com/uc?id=%s&export=download\", id))\n\n# Filter out rows with non-numeric values in the 'streams' column\nfiltered_spotify_data &lt;- total_list %&gt;%\n   filter(!grepl(\"[a-zA-Z]\", streams))\n\n# Sort the filtered dataset by the 'streams' column in descending order\nsorted_filtered_spotify_data &lt;- filtered_spotify_data %&gt;%\n  arrange(desc(streams))\n\n# Create HTML view of Dataset\nDT::datatable(head(sorted_filtered_spotify_data), editable = list(\n  target = 'row', disable = list(columns = c(1, 3, 4))\n))\n\n\n\n\n\n\n\n\n\n2.0.2 Exercises\n\nclass(filtered_spotify_data$streams)\n\n[1] \"character\"\n\n# Convert character to numeric , original r\nfiltered_spotify_data$streams &lt;- as.numeric(filtered_spotify_data$streams)\n\n# 1. The mean of all streams in the dataset\nmean_streams &lt;- filtered_spotify_data %&gt;%\n  summarise(mean_streams = mean(streams, na.rm = TRUE))\n\nprint(mean_streams)\n\n  mean_streams\n1    514137425\n\n# 2. The mean of streams for each month\nmonth_names &lt;- month.name\n\nmean_streams_by_month &lt;- filtered_spotify_data %&gt;%\n  group_by(released_month) %&gt;%\n  summarise(mean_streams = mean(streams, na.rm = TRUE)) %&gt;%\n  mutate(released_month = month_names[as.numeric(released_month)])\n\nprint(mean_streams_by_month)\n\n# A tibble: 12 × 2\n   released_month mean_streams\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 January          727506137.\n 2 February         353153502.\n 3 March            477052609.\n 4 April            404145980.\n 5 May              415669054.\n 6 June             410044671.\n 7 July             482176478.\n 8 August           631265701.\n 9 September        734644353.\n10 October          588902402.\n11 November         552592271.\n12 December         369573392.\n\n# 3. Transmute mutate example\nspotify_data_millions &lt;- filtered_spotify_data %&gt;%\n  mutate(streams_in_millions = streams / 1000000) %&gt;%\n  transmute(track_name, artist.s._name, released_year, streams_in_millions) %&gt;%\n  arrange(desc(streams_in_millions))\n\nprint(head(spotify_data_millions))\n\n                                     track_name        artist.s._name\n1                               Blinding Lights            The Weeknd\n2                                  Shape of You            Ed Sheeran\n3                             Someone You Loved         Lewis Capaldi\n4                                  Dance Monkey           Tones and I\n5 Sunflower - Spider-Man: Into the Spider-Verse Post Malone, Swae Lee\n6                                     One Dance   Drake, WizKid, Kyla\n  released_year streams_in_millions\n1          2019            3703.895\n2          2017            3562.544\n3          2018            2887.242\n4          2019            2864.792\n5          2018            2808.097\n6          2016            2713.922"
  },
  {
    "objectID": "inClass2.html",
    "href": "inClass2.html",
    "title": "3  In Class 2",
    "section": "",
    "text": "3.0.1 Most Streamed Spotify Songs 2023\nThe ‘Most Streamed Spotify Songs 2023’ dataset is a fantastic choice for class demos. It offers insights into the music industry by showcasing popular songs and their attributes. Students can explore the science of hit songs, understand their impact through social media analysis, and learn about cross-platform music data. It’s engaging and up-to-date, making it an excellent teaching resource.\n\n# Use Google Drive to connect Dataset with library of 'googleDrive'\nid &lt;- \"1nM0Cr-gVFF9UjvxSZXHNfFnrDWIUoTHi\"\ntotal_list &lt;- read.csv(sprintf(\"https://docs.google.com/uc?id=%s&export=download\", id))\n\n# Filter out rows with non-numeric values in the 'streams' column\nfiltered_spotify_data &lt;- total_list %&gt;%\n   filter(!grepl(\"[a-zA-Z]\", streams))\n\n# Sort the filtered dataset by the 'streams' column in descending order\nsorted_filtered_spotify_data &lt;- filtered_spotify_data %&gt;%\n  arrange(desc(streams))\n\n# Create HTML view of Dataset\nDT::datatable(head(sorted_filtered_spotify_data), editable = list(\n  target = 'row', disable = list(columns = c(1, 3, 4))\n))\n\n\n\n\n\n\n\n\n\n3.0.2 Exercises for GGPLOT2\nPractising the GGPLOT2, visualize Spotify data with three exercises. First, a bar chart illustrates the distribution of musical features. Second, a line plot depicts stream trends over time. Lastly, a dot plot showcases the top 10 artists with the highest streams, offering insights into their popularity.\n\nclass(filtered_spotify_data$streams)\n\n[1] \"character\"\n\n# Convert character to numeric , original r\nfiltered_spotify_data$streams &lt;- as.numeric(filtered_spotify_data$streams)\n\n# 1. Distribution of Musical Features\n\nordered_filtered_data &lt;- filtered_spotify_data[order(-filtered_spotify_data$streams), ]\n\ntop100_streams_data &lt;- head(ordered_filtered_data, 100)\n\ngathered_top100_data &lt;- gather(top100_streams_data, key = \"feature\", value = \"percentage\",\n                                danceability_., valence_., energy_., acousticness_., instrumentalness_.)\n\nggplot(gathered_top100_data, aes(x = feature, y = percentage, fill = feature)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Distribution of Musical Features (Top 100 Streams)\",\n       x = \"Musical Features\",\n       y = \"Percentage\",\n       fill = \"Feature\") +\n  theme_minimal()\n\n\n\n# 2. Line Plot for Streams over Time\nggplot(filtered_spotify_data, aes(x = released_year, y = streams, group = 1)) +\n  geom_line(color = \"green\") +\n  labs(title = \"Line Plot for Streams over Time\",\n       x = \"Released Year\",\n       y = \"Streams\")\n\n\n\n# 3. Dot Plot for Top 10 Artists with Highest Streams\n\ntop_artists &lt;- filtered_spotify_data %&gt;%\n  group_by(artist.s._name) %&gt;%\n  summarise(total_streams = sum(streams)) %&gt;%\n  top_n(10, total_streams)\n\nggplot(top_artists, aes(x = reorder(artist.s._name, total_streams), y = total_streams, fill = artist.s._name)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 2) +\n  labs(title = \"Dot Plot for Top 10 Artists with Highest Streams\",\n       x = \"Artist\",\n       y = \"Total Streams\",\n       fill = \"Artist\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "assignment_shiny.html#shiny-app-spotify-data-exploration",
    "href": "assignment_shiny.html#shiny-app-spotify-data-exploration",
    "title": "4  Assignment: Shiny",
    "section": "4.1 Shiny App: Spotify Data Exploration",
    "text": "4.1 Shiny App: Spotify Data Exploration\nThis Shiny app explores Spotify data, focusing on trends in various music features over the years. The data source is a dataset containing information on streams, release years, and different music attributes.\nYou can run my shiny app using the following command.\n\nlibrary(shiny)\nshiny::runGitHub(\"pjournal/mef07-sezerlucianturkmen\", subdir = \"/app/app.R\")"
  },
  {
    "objectID": "assignment_shiny.html#shiny-spotify-link",
    "href": "assignment_shiny.html#shiny-spotify-link",
    "title": "4  Assignment: Shiny",
    "section": "4.2 Shiny Spotify Link",
    "text": "4.2 Shiny Spotify Link\nYou can run my Shiny app by clicking on the image below:\n\n\n\n\n\n\nAlso, you can try the app on Shinyapps."
  },
  {
    "objectID": "ORAssignment.html",
    "href": "ORAssignment.html",
    "title": "5  OR Assignment",
    "section": "",
    "text": "6 Optimizing Gate and Stand Assignments at Copenhagen Airport"
  },
  {
    "objectID": "ORAssignment.html#business-situation",
    "href": "ORAssignment.html#business-situation",
    "title": "5  OR Assignment",
    "section": "6.1 Business Situation",
    "text": "6.1 Business Situation\nCopenhagen Airport (CPH) faced a challenge similar to finding parking in a busy lot, but with aircraft stands. The airport needed to maximize the use of existing stands and plan for future expansion cost-effectively. Each new stand cost $10M USD (30M-50M DDK), prompting the need for a precise optimization strategy."
  },
  {
    "objectID": "ORAssignment.html#building-the-model",
    "href": "ORAssignment.html#building-the-model",
    "title": "5  OR Assignment",
    "section": "6.2 Building the Model",
    "text": "6.2 Building the Model\nThe Operational Research (OR) team at CPH developed a detailed optimization model, moving away from traditional top-down analyses. They incorporated operational requirements and restrictions into the model, considering hard constraints (specific aircraft types for certain stands) and softer preferences (airlines’ proximity preferences). The mathematical model was constructed in two months using the Gurobi C++ API."
  },
  {
    "objectID": "ORAssignment.html#results",
    "href": "ORAssignment.html#results",
    "title": "5  OR Assignment",
    "section": "6.3 Results",
    "text": "6.3 Results\nGurobi efficiently found solutions, offering valuable insights and identifying areas for refinement. The model, with additional operational details, enabled quick what-if analyses for decision trade-offs. The optimized plan recommended by the model led to investments totaling over $70M USD ($400M DDK) in new stands and gates."
  },
  {
    "objectID": "ORAssignment.html#benefits",
    "href": "ORAssignment.html#benefits",
    "title": "5  OR Assignment",
    "section": "6.4 Benefits",
    "text": "6.4 Benefits\n\nCost Savings: Gurobi’s model led to $79M USD in savings by optimizing stand assignments and guiding strategic investments.\nEfficient Utilization: Gurobi maximized existing stand usage, ensuring precise allocation based on aircraft types and airlines’ preferences for improved efficiency.\nQuick Decision-Making: Gurobi’s fast solutions facilitated swift decision-making, enabling the team to adapt to changing scenarios effectively.\nFlexibility and Adaptability: The model easily adapted to operational changes, accommodating soft preferences and ensuring efficient decision-making.\nStrategic Planning: The optimized plan not only addressed current challenges but also served as a guide for future expansions, ensuring long-term sustainability.\nUser-Friendly Interface: Leveraging Excel made the process user-friendly, enhancing collaboration and integration into existing workflows."
  },
  {
    "objectID": "ORAssignment.html#reference",
    "href": "ORAssignment.html#reference",
    "title": "5  OR Assignment",
    "section": "6.5 Reference",
    "text": "6.5 Reference\nCopenhagen Airport Case Study"
  },
  {
    "objectID": "preprocesseddata.html#yellow-submarines-project-proposal-on-wits-data",
    "href": "preprocesseddata.html#yellow-submarines-project-proposal-on-wits-data",
    "title": "6  Preprocessed Data",
    "section": "6.1 Yellow Submarine’s Project Proposal on WITS data",
    "text": "6.1 Yellow Submarine’s Project Proposal on WITS data\nThe choice of this domain stems from the significance of Turkey’s role in the global trade landscape between 2002-2020. Before settling on the WITS database, we reviewed existing solutions, including the Harvard University Atlas project and Visualizations from the Observatory of Economic Complexity.)\nWe have decided on this database considering the primary sources that include:\na. Exports&Imports: Analyze composition of exports and imports data by complexity for each commodity\nb. Trade Partnerships: Utilize international trade databases to identify Turkey’s major trade partner and assess trade volumes\nc. Additional Insights: Explore supplementary datasets as needed to uncover specific aspects of Turkey’s trade dynamics, such as tariffs and distances data to map out key trading routes, highlighting the connectivity and strategic importance of Turkey in global trade.\nWith this project, we aim to produce a comprehensive report that synthesizes the findings into a coherent narrative. Visualizations, charts, and infographics will be employed to enhance the accessibility of the data."
  },
  {
    "objectID": "preprocesseddata.html#dataset-installation-and-preprocessing",
    "href": "preprocesseddata.html#dataset-installation-and-preprocessing",
    "title": "6  Preprocessed Data",
    "section": "6.2 Dataset Installation and Preprocessing",
    "text": "6.2 Dataset Installation and Preprocessing\n\n6.2.1 Necessary Source Packages\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tradestatistics)\n\nWarning: package 'tradestatistics' was built under R version 4.3.2\n\nlibrary(tibble)\n\n\n\n6.2.2 Datasets\nThe data retrieval and table creation were facilitated using the ‘tradestatistics’ package developed by a group of developers led by Mauricio Vargas. This package streamlines the process of fetching data from api.tradestatistics.io and generates tables that are convenient for analytical purposes. We used otc_create_tidy_data for our analysis\nDataSet 1: This data set shows Turkey’s export and import values with product categories & related commodities’ names & codes only\nDataSet 2: This data set shows Turkey’s and its partners’ export and import values with product categories & related commodities’ names & codes\n\n#DataSet 1: \n\nwits_turkey_data_only &lt;- ots_create_tidy_data(\n  years = 2002:2020,\n  reporters = \"tur\",\n  table = \"yrc\"\n)\nhead(wits_turkey_data_only)\n\n#DataSet 2: \n\nwits_turkey_data_with_partners &lt;- ots_create_tidy_data(\n  years = 2002:2020,\n  reporters = \"tur\",\n  table = \"yrpc\"\n)\nhead(wits_turkey_data_with_partners)\n\n#Combine the datasets into a list\ncombined_datasets &lt;- list(\n  wits_turkey_data_only = wits_turkey_data_only,\n  wits_turkey_data_with_partners = wits_turkey_data_with_partners\n)"
  },
  {
    "objectID": "preprocesseddata.html#creating-rds-file",
    "href": "preprocesseddata.html#creating-rds-file",
    "title": "6  Preprocessed Data",
    "section": "6.3 Creating RDS file",
    "text": "6.3 Creating RDS file\n\nsaveRDS(combined_datasets, file = \"wits_data.rds\")\n\nThe created .Rdata file can be reached through RDS Link"
  },
  {
    "objectID": "preprocesseddata.html#monitoring-the-data",
    "href": "preprocesseddata.html#monitoring-the-data",
    "title": "6  Preprocessed Data",
    "section": "6.4 Monitoring the Data",
    "text": "6.4 Monitoring the Data\n\nlibrary(DT)\n\nWarning: package 'DT' was built under R version 4.3.2\n\nloaded_datasets &lt;- readRDS(\"wits_data.rds\")\nread_wits_turkey_data_only &lt;- loaded_datasets$wits_turkey_data_only\nread_wits_turkey_data_with_partners &lt;- loaded_datasets$wits_turkey_data_with_partners\n\nDT::datatable(head(read_wits_turkey_data_only),editable = list(\n  target = 'row', disable = list(columns = c(1, 3, 4))\n))\n\n\n\n\n\nDT::datatable(head(read_wits_turkey_data_with_partners), editable = list(\n  target = 'row', disable = list(columns = c(1, 3, 4))\n))"
  },
  {
    "objectID": "preprocesseddata.html#references",
    "href": "preprocesseddata.html#references",
    "title": "6  Preprocessed Data",
    "section": "6.5 References",
    "text": "6.5 References\nBasic examples listed in this link were useful during creating RDS and monitoring data tables"
  },
  {
    "objectID": "final.html#part-i-short-and-simple",
    "href": "final.html#part-i-short-and-simple",
    "title": "7  Final Assignment",
    "section": "PART I: Short and Simple",
    "text": "PART I: Short and Simple\n\nQUESTION 1\nImagine a crowded bus – outside, I might say, “Hey, move a bit, there’s space inside!” But in the middle, I’d say, “Come on, it’s already full, no more space!” This mirrors how people want AI but fear holds them back, like they’re stuck in the middle of the bus. Personally, I see AI as a helpful tool, like a colleague or an artist’s tool. Too many rules might limit its potential, like restricting a person’s freedom. Regulations should stop AI misuse without stifling innovation. AI saves time, letting us focus on important things. Balancing regulations is crucial; too many can hinder progress. Punishing misuse is key to safeguarding against harm. AI, used responsibly, enhances our abilities. In conclusion, thoughtful regulations are needed, avoiding stifling innovation while recognizing AI’s transformative role. Responsible development ensures AI is a positive force, helping us navigate the modern world.\n\n\nQUESTION 2\nDefine Objectives: Clearly outline the goals and expected outcomes of the automated process.\nUnderstand Current Process: Analyze the existing manual process in Excel to identify key steps and dependencies.\nData Exploration: Examine the data structure, identifying patterns and potential challenges.\nTechnology Assessment: Evaluate tools and platforms suitable for automation, considering scalability and compatibility.\nIterative Development: Incrementally enhance the automation, addressing specific team needs and challenges.\nUser Feedback: Regularly involve the team for feedback, ensuring alignment with workflow.\nTesting and Validation: Rigorously test the automation for accuracy, reliability, and efficiency.\nDocumentation: Clearly document the automated process, aiding future troubleshooting and improvements.\nImplementation: Roll out the automated solution, providing necessary training and support for a seamless transition.\n\n\nQUESTION 3\nIn the USA, the presidential election outcome is determined by the Electoral College system, where each state contributes electoral votes based on its population. This approach emphasizes the significance of winning states with higher electoral votes rather than the raw popular vote count. The plot I chose succinctly captures this essence, displaying the total electoral votes won by each candidate in a bar chart. This visualization provides a clear, state-centric perspective on the election, showcasing the distribution of electoral power. In the 2016 election, despite winning fewer individual votes, Trump secured crucial states with higher electoral votes, leading to his overall victory. Therefore, this chart effectively highlights the pivotal role of state-level electoral dynamics in determining the ultimate winner of the presidential race.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(dslabs)\n# Load the dataset containing results of the US election 2016\ndata &lt;- dslabs::results_us_election_2016\n# Create a new 'candidate' based on the comparison of votes for Trump, Clinton\ndata &lt;- data %&gt;%  \n  mutate(candidate = case_when(\n    trump &gt; clinton & trump &gt; others ~ 'trump',\n    clinton &gt; trump & clinton &gt; others ~ 'clinton',\n    others &gt; clinton & others &gt; trump ~ 'others', TRUE ~ 'tie'))\n# Group the data by candidate and calculate the total electoral votes for each\ndata_percentage &lt;- data %&gt;% group_by(candidate) %&gt;%\n  summarise(total_electoral_votes = sum(electoral_votes))\n# Define colors for Trump, Clinton, and others\ntrump_c &lt;- \"#FF6F61\"  # Soft red\nclinton_c &lt;- \"#00B7Da\"  # Coral blue\n# Create a bar plot  to visualize the total electoral votes won by each candidate\nggplot(data_percentage, aes(x=candidate,y=total_electoral_votes,fill=candidate)) +\n  geom_bar(stat = \"identity\") +  \n  geom_text(aes(label = total_electoral_votes), \n            position = position_stack(vjust = 0.5), size = 5) + \n  scale_fill_manual(values = c(trump =trump_c,clinton=clinton_c,others =\"gray\")) +\n  labs(title = \"Electoral Votes Won by Each Candidate\",\n       x = \"Candidate\", y = \"Total Electoral Votes\")"
  },
  {
    "objectID": "final.html#part-ii-extending-group-project",
    "href": "final.html#part-ii-extending-group-project",
    "title": "7  Final Assignment",
    "section": "PART II : Extending Group Project",
    "text": "PART II : Extending Group Project\n\nParis Agreement: Turkey-Europe Trade and Carbon Licensing\nDescription\nIn assessing the impact of Carbon Footprint Licensing on Turkey’s trade relations with Europe, I find a critical juncture for our nation’s economic landscape. As Turkey’s major partner, the European Union demands compliance with the Paris Climate Agreement, necessitating a focus on carbon emissions in production processes. However, Turkey, in its pivotal trade with Europe, lacks preparedness for this shift, neglecting the Paris Climate Agreement’s mandates. This oversight jeopardizes our trade balance and economic stability. The impending requirement for carbon footprint licenses adds a layer of complexity, demanding urgent attention to align with European environmental standards and ensure the longevity of our trade relationships.Key Takeaways :\nEuropean Union Dominance: Europe, especially the European Union, holds a crucial role in Turkey’s trade dynamics, contributing to 48.1% of the total exports.\nClimate Rules Imposition: Europe’s adoption of climate regulations, including the requirement for carbon footprint data during imports, poses a significant challenge for Turkey. This will impact the exporting process, necessitating compliance with European environmental standards.\nLack of Regulation and R&D: Turkey’s current state of missing regulations and insufficient investment in research and development (R&D) leaves it unprepared for the upcoming European mandates. Rapid adaptation is crucial to avoid potential trade disruptions.\nPotential Financial Burden: The imposition of extra taxes on products with a non-zero carbon footprint creates an additional financial burden for Turkish exporters. This adds pressure to align with European standards and invest in sustainable practices to mitigate increased expenses.\nUrgent Need for Adaptation: Given the imminent changes in European import regulations, Turkey must prioritize the implementation of necessary regulations, invest in R&D, and develop strategies to achieve carbon neutrality in its exports to maintain a competitive edge in its major trading partner, Europe.\n\n\nCode\neuropean_trade_data &lt;- read_wits_turkey_data_with_partners %&gt;%\n  mutate(is_european = ifelse(partner_name %in% european_countries_names,\n                              \"European\", \"Non-European\"))\nyearly_trade_europe &lt;- european_trade_data %&gt;%\ngroup_by(is_european) %&gt;%\nsummarize(total_export=sum(trade_value_usd_exp),total_import=sum(trade_value_usd_imp))\ntotal_export &lt;- sum(yearly_trade_europe$total_export)\n# Calculate the percentage of total export for each group\nyearly_trade_europe &lt;- yearly_trade_europe %&gt;%\n  mutate(percentage_export = ifelse(total_export &gt; 0,\n                                     total_export / sum(total_export) * 100, 0))\n# Create a bar plot to visualize the trade for European and Non-European partners\nggplot(data = yearly_trade_europe, aes(x = is_european)) +\n  geom_bar(aes(y=total_export, fill = \"Export\"),stat=\"identity\",position =\"dodge\")+\n  geom_bar(aes(y=-total_import,fill=\"Import\"),stat=\"identity\", position = \"dodge\")+\n  geom_text(aes(label = paste(round(percentage_export, 1), \"%\"), y = total_export),\n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"European & Non-European Trade Analysis\",\n       x = \"Partner Group\", y = \"Total Trade Value (USD)\") +\n  scale_fill_manual(values = c(\"Export\" = \"#00B466\", \"Import\" = \"#FF6F61\"),\n                    name = \"\", labels = c(\"Export\", \"Import\")) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  theme(axis.text.x=element_text(angle=90,hjust = 0.5),legend.position =\"right\")+\n  scale_y_continuous(labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\nEuropean & Non-European Trade Analysis The bar plot illustrates the trade dynamics between Turkey and European and non-European partners. Notably, 48.1% of Turkey’s total trade is associated with European countries. This significant portion of trade indicates a substantial economic relationship with Europe.\nIt is crucial to acknowledge that this trading balance holds strategic importance, especially considering the requirements of the Paris Agreement. Turkey’s export activities, constituting almost half of its total trade, could be impacted by the evolving landscape of carbon credit requirements. Failure to align with the Paris Agreement’s carbon reduction standards might pose challenges to Turkey’s export economy, potentially affecting its overall income.\nUnderstanding and adapting to the changing dynamics of international trade, particularly in the context of environmental regulations, will be essential for Turkey to navigate potential challenges and sustain a resilient economic outlook.\n\n\nCode\neuropean_trade_data &lt;- read_wits_turkey_data_with_partners %&gt;%\n  filter(partner_name %in% european_countries_names)\nyearly_trade_europe &lt;- european_trade_data %&gt;%group_by(partner_name) %&gt;%\nsummarize(total_export=sum(trade_value_usd_exp),total_import=sum(trade_value_usd_imp)) \n# Create a bar plot to visualize the trade position of Turkey with Europe\nggplot(data = yearly_trade_europe, aes(x = partner_name)) +\n  geom_bar(aes(y = total_export,fill=\"Export\"),stat =\"identity\",position=\"dodge\")+\n  geom_bar(aes(y = -total_import,fill=\"Import\"),stat =\"identity\",position=\"dodge\")+\n  labs(title = \"Trade Position of Turkey with European Countries\",\n       x = \"European Partner\", y = \"Total Trade Value (USD)\") +\n  scale_fill_manual(values = c(\"Export\" = \"#00B466\", \"Import\" = \"#FF6F61\"),\n                    name = \"\", labels = c(\"Export\", \"Import\")) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 0.5),\nlegend.position=\"right\")+scale_y_continuous(labels=scales::comma_format(scale=1e-6))\n\n\n\n\n\nTrade Analysis with European Countries The bar plot delves into Turkey’s trade dynamics with specific European nations. Notably, Italy, Spain, France, Germany, and the United Kingdom emerge as key players influencing Turkey’s trade balance. These major partners, however, face environmental restrictions, impacting sectors like construction.\nTo thrive amidst this, Turkey must prioritize carbon footprint calculations and adopt sustainable practices in exports. While challenges persist, diversifying trading partners and exploring new markets become crucial. Strategic planning for sustainable trade practices and expansion efforts will be key to navigating evolving international trade dynamics effectively.\n\n\nCode\n# Read the trade data with partners for Turkey and filter for European countries\neuropean_trade_data &lt;- read_wits_turkey_data_with_partners %&gt;%\n  filter(partner_name %in% european_countries_names)\ntotal_exports &lt;- european_trade_data %&gt;% group_by(year) %&gt;%\n  summarize(total_exports = sum(trade_value_usd_exp))\ntotal_imports &lt;- european_trade_data %&gt;% group_by(year) %&gt;%\n  summarize(total_imports = sum(trade_value_usd_imp))\n# visualize Turkey's exports and imports with European countries over the years\nggplot() +\n  geom_line(data=total_exports,aes(x=year,y=total_exports,color=\"Exports\"),size=1.5)+\n  geom_line(data=total_imports,aes(x=year,y=total_imports,color=\"Imports\"),size=1.5)+\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  labs(title=\"Turkey's Exports and Imports with European Countries Over the Years\",\n       x = \"Year\", y = \"Trade Value (mn USD)\") +\n  scale_y_continuous(labels = scales::comma_format(scale = 1e-6)) +\n  scale_color_manual(values = c(\"Exports\" = \"#00B466\", \"Imports\" = \"#FF6F61\"))\n\n\n\n\n\nTurkey’s Trade Dynamics with European Countries: A Post-2018 Perspective The line plot illustrates Turkey’s evolving trade relationship with European countries over the years. A notable shift is observed post-2018, where Turkey transitions from being a net importer to achieving a trade surplus, with exports surpassing imports. This shift is indicative of a positive trend in revenue-generating trade activities.\nWhile acknowledging the potential impact of the 2019 pandemic on global trade, it is crucial to recognize the resilience showcased by Turkey in maintaining a favorable trade balance with European partners.\nSustaining this positive trade atmosphere beyond the pandemic becomes paramount. To ensure continued success, Turkey should focus on strategic measures such as diversification of export markets, exploration of new trading partners, and the implementation of sustainable practices. Maintaining this advantageous position in European markets will contribute significantly to Turkey’s economic stability and resilience.\nConclusion\nIn conclusion, Turkey’s strong trade ties with Europe, particularly the European Union, face a critical juncture due to impending climate regulations. With nearly half of Turkey’s exports directed to Europe, the lack of preparedness, missing regulations, and limited investment in R&D pose significant challenges. The necessity for carbon footprint data during imports, coupled with potential extra taxes for non-compliance, underscores the urgency for Turkey to adapt swiftly. To safeguard economic interests and trade relations, a proactive approach involving regulatory reforms, increased research, and sustainable practices is imperative to align with European environmental standards and ensure long-term viability in the evolving global trade landscape.\nReferences\nCommission President advances global cooperation on carbon pricing in high-level event at COP28\nWorld Integrated Trade Solution (World Bank)"
  },
  {
    "objectID": "final.html#part-iii-welcome-to-real-life",
    "href": "final.html#part-iii-welcome-to-real-life",
    "title": "7  Final Assignment",
    "section": "PART III : Welcome to Real Life",
    "text": "PART III : Welcome to Real Life\nIn this data analysis phase, participants navigate the challenges of gathering and cleaning data from the EPIAS Transparency Platform, specifically focusing on power plant events during November 2023. After compiling and saving the data in an .RData file, a succinct Exploratory Data Analysis (EDA) is conducted, culminating in concise insights. Collaboration is encouraged for dataset creation, and the opportunity to extend findings into a data blog post is presented.\n1 Load libraries\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\n2 Load the dataset an d Check the structure\n\n\nCode\n#Read the data set from RData\nload(\"data_power_plants.RData\")\npart_three_data &lt;- data_power_plants\n#Control the data type and column names\n#str(part_three_data)\n#summary(part_three_data)\n\n\n3 Clean and Edit the dataset\n\n\nCode\ncolnames(part_three_data) &lt;- c(\"Organization_Name\", \"Power_Plant_Name\", \n  \"DAB\", \"Event_Start_Date\", \"Event_End_Date\", \"Installed_Capacity\", \n  \"Capacity_During_Event\",\"Reason\", \"Maintenance_Breakdown\")\npart_three_data$Installed_Capacity &lt;- as.numeric(part_three_data$Installed_Capacity)\npart_three_data$Capacity_During_Event &lt;- as.numeric(part_three_data$Capacity_During_Event)\nsummary(part_three_data)\n\n\n Organization_Name  Power_Plant_Name       DAB           \n Length:2304        Length:2304        Length:2304       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n Event_Start_Date                 Event_End_Date                  \n Min.   :2023-11-01 00:00:00.00   Min.   :2023-11-01 03:59:00.00  \n 1st Qu.:2023-11-10 00:00:00.00   1st Qu.:2023-11-10 07:59:00.00  \n Median :2023-11-18 08:41:00.00   Median :2023-11-18 22:59:00.00  \n Mean   :2023-11-17 04:53:22.42   Mean   :2023-11-17 17:20:56.60  \n 3rd Qu.:2023-11-24 11:30:45.00   3rd Qu.:2023-11-24 23:59:00.00  \n Max.   :2023-11-30 23:38:00.00   Max.   :2024-01-13 23:59:00.00  \n Installed_Capacity Capacity_During_Event    Reason         \n Min.   :   0.2     Min.   :   0.0        Length:2304       \n 1st Qu.:  27.5     1st Qu.:   0.0        Class :character  \n Median : 150.0     Median :  25.0        Mode  :character  \n Mean   : 294.2     Mean   : 117.5                          \n 3rd Qu.: 420.0     3rd Qu.: 154.0                          \n Max.   :2405.0     Max.   :1800.0                          \n Maintenance_Breakdown\n Length:2304          \n Class :character     \n Mode  :character     \n                      \n                      \n                      \n\n\n4 Exploratory Data Analysis (EDA)\nKey Findings\nIn November 2023, Turkey’s power plants exhibited distinct performance characteristics based on their sustainability. Green Power Plants, constituting 46% of the nation’s capacity (according to “Energy Market Regulatory Authority), experienced a capacity reduction of 23.5%, showcasing relatively stable operational efficiency. The maintenance issues for these plants are yet to be precisely quantified. Conversely, Non-Green Power Plants, representing 54% of the total capacity, encountered a higher capacity reduction of 76.5%, indicating a greater susceptibility to operational disruptions. Further analysis is needed to determine the exact percentage of maintenance issues for non-green plants. This key comparison underscores the significance of sustainability in mitigating capacity reduction challenges, with green power plants demonstrating a more resilient operational profile during November 2023. Adjustments and detailed investigations based on specific dataset characteristics can provide further insights into enhancing the overall reliability and sustainability of Turkey’s energy infrastructure.\n\n\nCode\ndata &lt;- part_three_data %&gt;%\n  mutate(Environmentally_Friendly = grepl(\"HES|REG|REG\\\\.|GES\",\n              Power_Plant_Name, ignore.case = TRUE)) %&gt;%\n  group_by(Environmentally_Friendly) %&gt;%\n  summarise(Total_Capacity = sum(Installed_Capacity))\ndata &lt;- data %&gt;%\n  mutate(Percentage = Total_Capacity / sum(Total_Capacity) * 100,\n  Category = ifelse(Environmentally_Friendly, \"Sustainable\", \"Non-Sustainable\"))\nggplot(data, aes(x=factor(Category),y=Percentage,fill=Category))+\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Percentage)), vjust = -0.5, size = 3)+ \n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  labs(title = \"Percentage of Electric Capacity in Turkey (November)\",\n       x = \"Sustainable Energy Plants\", y = \"Percentage\") +\nscale_fill_manual(values=c(\"Sustainable\"=\"#00B466\",\"Non-Sustainable\"=\"#FF6F61\")) \n\n\n\n\n\nKey Findings\nThe daily capacity reduction in Turkey for November averaged less than 20,000, showcasing a generally stable operational landscape. However, November 20 emerged as a notable outlier, witnessing a substantial peak with capacity reduction exceeding 40,000. This spike indicates a specific day of heightened challenges in maintaining consistent electrical output.\n\n\nCode\nnovember_data &lt;- part_three_data %&gt;%\n  mutate(Event_Start_Date = as.Date(Event_Start_Date)) %&gt;%\n  filter(format(Event_Start_Date, \"%Y-%m\") == \"2023-11\")\ndaily_capacity_change &lt;- november_data %&gt;%\n  group_by(day = as.Date(Event_Start_Date)) %&gt;%\n  summarise(Daily_Capacity_Change = sum(Installed_Capacity)-sum(Capacity_During_Event))\nggplot(daily_capacity_change, aes(x = day, y = Daily_Capacity_Change)) +\n  geom_line(size = 1.5) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  labs(title = \"Daily Capacity Reduction in Turkey (November)\",\n       x = \"Date\",       y = \"Daily Capacity Reduction\")\n\n\n\n\n\nKey Findings\nBreakdowns emerged as the most prevalent issue, causing capacity to plummet to zero, signifying complete halts in power generation. On an average daily basis, around 60 events occurred, with November 20 registering the highest count, followed closely by the end of the month. Breakdowns played a pivotal role in these events, surpassing maintenance issues in frequency and impact.\n\n\nCode\nnovember_data &lt;- part_three_data %&gt;%\n  mutate(Event_Start_Date = as.Date(Event_Start_Date)) %&gt;%\n  filter(format(Event_Start_Date, \"%Y-%m\") == \"2023-11\")\ndaily_event_counts &lt;- november_data %&gt;%\n  group_by(day = as.Date(Event_Start_Date)) %&gt;%\n  summarise(Stop = sum(Capacity_During_Event == 0),\n            Maintenance = sum(Maintenance_Breakdown == \"M\"&Capacity_During_Event&gt;0),\n            Breakdown = sum(Maintenance_Breakdown == \"B\"&Capacity_During_Event&gt;0))\ndaily_event_counts_long &lt;- daily_event_counts %&gt;%\n  tidyr::gather(key = \"Event_Type\", value = \"Count\", -day)\nggplot(daily_event_counts_long, aes(x = day, y = Count, fill = Event_Type)) +\n  geom_bar(stat = \"identity\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"black\", size = 0.5) +\n  labs(title = \"Daily Event Counts in Turkey (November)\",\n       x = \"Date\", y = \"Event Count\", fill = \"Event Type\") +\n  scale_fill_manual(values = c(\"Stop\" = \"#FF6F61\", \"Maintenance\" = \"#fcb953\", \n      \"Breakdown\" = \"#40E0D0\")) + theme(legend.position = \"bottom\")\n\n\n\n\n\nReferences\nEPIAS Transperency Platform\nNew about percentage of Green Energy Plants in Turkey"
  }
]